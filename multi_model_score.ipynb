{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gzip\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load and Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>risk_score_t</th>\n",
       "      <th>program_enrolled_t</th>\n",
       "      <th>cost_t</th>\n",
       "      <th>cost_avoidable_t</th>\n",
       "      <th>bps_mean_t</th>\n",
       "      <th>ghba1c_mean_t</th>\n",
       "      <th>hct_mean_t</th>\n",
       "      <th>cre_mean_t</th>\n",
       "      <th>ldl_mean_t</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>trig_min-high_tm1</th>\n",
       "      <th>trig_min-normal_tm1</th>\n",
       "      <th>trig_mean-low_tm1</th>\n",
       "      <th>trig_mean-high_tm1</th>\n",
       "      <th>trig_mean-normal_tm1</th>\n",
       "      <th>trig_max-low_tm1</th>\n",
       "      <th>trig_max-high_tm1</th>\n",
       "      <th>trig_max-normal_tm1</th>\n",
       "      <th>gagne_sum_tm1</th>\n",
       "      <th>gagne_sum_t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.987430</td>\n",
       "      <td>0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>194.0</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.677934</td>\n",
       "      <td>0</td>\n",
       "      <td>2600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>40.4</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.407678</td>\n",
       "      <td>0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798369</td>\n",
       "      <td>0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.513165</td>\n",
       "      <td>0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.1</td>\n",
       "      <td>1.303333</td>\n",
       "      <td>53.0</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   risk_score_t  program_enrolled_t  cost_t  cost_avoidable_t  bps_mean_t  \\\n",
       "0      1.987430                   0  1200.0               0.0         NaN   \n",
       "1      7.677934                   0  2600.0               0.0       119.0   \n",
       "2      0.407678                   0   500.0               0.0         NaN   \n",
       "3      0.798369                   0  1300.0               0.0       117.0   \n",
       "4     17.513165                   0  1100.0               0.0       116.0   \n",
       "\n",
       "   ghba1c_mean_t  hct_mean_t  cre_mean_t  ldl_mean_t   race  ...  \\\n",
       "0            5.4         NaN    1.110000       194.0  white  ...   \n",
       "1            5.5        40.4    0.860000        93.0  white  ...   \n",
       "2            NaN         NaN         NaN         NaN  white  ...   \n",
       "3            NaN         NaN         NaN         NaN  white  ...   \n",
       "4            NaN        34.1    1.303333        53.0  white  ...   \n",
       "\n",
       "   trig_min-high_tm1  trig_min-normal_tm1  trig_mean-low_tm1  \\\n",
       "0                  0                    0                  0   \n",
       "1                  0                    1                  0   \n",
       "2                  0                    0                  0   \n",
       "3                  0                    0                  0   \n",
       "4                  0                    0                  0   \n",
       "\n",
       "   trig_mean-high_tm1  trig_mean-normal_tm1  trig_max-low_tm1  \\\n",
       "0                   0                     0                 0   \n",
       "1                   0                     1                 0   \n",
       "2                   0                     0                 0   \n",
       "3                   0                     0                 0   \n",
       "4                   0                     0                 0   \n",
       "\n",
       "   trig_max-high_tm1  trig_max-normal_tm1  gagne_sum_tm1  gagne_sum_t  \n",
       "0                  0                    0              0            0  \n",
       "1                  0                    1              4            3  \n",
       "2                  0                    0              0            0  \n",
       "3                  0                    0              0            0  \n",
       "4                  0                    0              1            1  \n",
       "\n",
       "[5 rows x 160 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data_path = 'data_new.csv'\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"The data file {data_path} does not exist.\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define age band columns and their corresponding numerical values\n",
    "age_band_columns = [\n",
    "    'dem_age_band_18-24_tm1', 'dem_age_band_25-34_tm1', 'dem_age_band_35-44_tm1',\n",
    "    'dem_age_band_45-54_tm1', 'dem_age_band_55-64_tm1', 'dem_age_band_65-74_tm1', 'dem_age_band_75+_tm1'\n",
    "]\n",
    "\n",
    "age_mapping = {\n",
    "    'dem_age_band_18-24_tm1': 21,\n",
    "    'dem_age_band_25-34_tm1': 30,\n",
    "    'dem_age_band_35-44_tm1': 40,\n",
    "    'dem_age_band_45-54_tm1': 50,\n",
    "    'dem_age_band_55-64_tm1': 60,\n",
    "    'dem_age_band_65-74_tm1': 70,\n",
    "    'dem_age_band_75+_tm1': 80\n",
    "}\n",
    "\n",
    "# Initialize 'age' with zeros\n",
    "df['age'] = 0\n",
    "\n",
    "# Assign age values based on the presence in each band\n",
    "for band, age_val in age_mapping.items():\n",
    "    if band in df.columns:\n",
    "        df['age'] += df[band] * age_val\n",
    "\n",
    "\n",
    "# Handle missing values by filling with median for numeric columns\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Encode the categorical variable 'race' if it is non-numeric\n",
    "if 'race' in df.columns and df['race'].dtype == 'object':\n",
    "    df['race'] = LabelEncoder().fit_transform(df['race'])\n",
    "\n",
    "\n",
    "# Define biomarker columns\n",
    "biomarker_columns = [\n",
    "    'cre_min-low_tm1', 'cre_min-high_tm1', 'cre_min-normal_tm1',\n",
    "    'cre_mean-low_tm1', 'cre_mean-high_tm1', 'cre_mean-normal_tm1',\n",
    "    'cre_max-low_tm1', 'cre_max-high_tm1', 'cre_max-normal_tm1',\n",
    "    'crp_min-low_tm1', 'crp_min-high_tm1', 'crp_min-normal_tm1',\n",
    "    'crp_mean-low_tm1', 'crp_mean-high_tm1', 'crp_mean-normal_tm1',\n",
    "    'crp_max-low_tm1', 'crp_max-high_tm1', 'crp_max-normal_tm1',\n",
    "    'esr_min-low_tm1', 'esr_min-high_tm1', 'esr_min-normal_tm1',\n",
    "    'esr_mean-low_tm1', 'esr_mean-high_tm1', 'esr_mean-normal_tm1',\n",
    "    'esr_max-low_tm1', 'esr_max-high_tm1', 'esr_max-normal_tm1',\n",
    "    'ghba1c_min-low_tm1', 'ghba1c_min-high_tm1', 'ghba1c_min-normal_tm1',\n",
    "    'ghba1c_mean-low_tm1', 'ghba1c_mean-high_tm1', 'ghba1c_mean-normal_tm1',\n",
    "    'ghba1c_max-low_tm1', 'ghba1c_max-high_tm1', 'ghba1c_max-normal_tm1',\n",
    "    'hct_min-low_tm1', 'hct_min-high_tm1', 'hct_min-normal_tm1',\n",
    "    'hct_mean-low_tm1', 'hct_mean-high_tm1', 'hct_mean-normal_tm1',\n",
    "    'hct_max-low_tm1', 'hct_max-high_tm1', 'hct_max-normal_tm1',\n",
    "    'ldl_min-low_tm1', 'ldl_min-high_tm1', 'ldl_min-normal_tm1',\n",
    "    'ldl-mean-low_tm1', 'ldl-mean-high_tm1', 'ldl-mean-normal_tm1',\n",
    "    'ldl_max-low_tm1', 'ldl_max-high_tm1', 'ldl_max-normal_tm1',\n",
    "    'nt_bnp_min-low_tm1', 'nt_bnp_min-high_tm1', 'nt_bnp_min-normal_tm1',\n",
    "    'nt_bnp_mean-low_tm1', 'nt_bnp_mean-high_tm1', 'nt_bnp_mean-normal_tm1',\n",
    "    'nt_bnp_max-low_tm1', 'nt_bnp_max-high_tm1', 'nt_bnp_max-normal_tm1',\n",
    "    'sodium_min-low_tm1', 'sodium_min-high_tm1', 'sodium_min-normal_tm1',\n",
    "    'sodium_mean-low_tm1', 'sodium_mean-high_tm1', 'sodium_mean-normal_tm1',\n",
    "    'sodium_max-low_tm1', 'sodium_max-high_tm1', 'sodium_max-normal_tm1',\n",
    "    'trig_min-low_tm1', 'trig_min-high_tm1', 'trig_min-normal_tm1',\n",
    "    'trig_mean-low_tm1', 'trig_mean-high_tm1', 'trig_mean-normal_tm1',\n",
    "    'trig_max-low_tm1', 'trig_max-high_tm1', 'trig_max-normal_tm1'\n",
    "]\n",
    "\n",
    "# Select columns where data is present (non-null values)\n",
    "non_empty_biomarker_columns = df[biomarker_columns].dropna(axis=1, how='all')\n",
    "\n",
    "# Randomly select 10 columns with data present\n",
    "if len(non_empty_biomarker_columns.columns) < 10:\n",
    "    random_10_biomarker_columns = non_empty_biomarker_columns.columns.tolist()\n",
    "    print(f\"Only {len(random_10_biomarker_columns)} biomarker columns available. Using all.\")\n",
    "else:\n",
    "    random_10_biomarker_columns = non_empty_biomarker_columns.sample(n=10, axis=1, random_state=42).columns.tolist()\n",
    "\n",
    "# Ensure selected biomarker columns exist in the DataFrame\n",
    "valid_biomarker_columns = [col for col in random_10_biomarker_columns if col in df.columns]\n",
    "\n",
    "# Create the 'biomarkers' column\n",
    "if valid_biomarker_columns:\n",
    "    # Example consolidation: set to 1 if any 'normal' biomarker is > 0\n",
    "    df['biomarkers'] = df[valid_biomarker_columns].apply(\n",
    "        lambda row: 1 if any('normal' in col and row[col] > 0 for col in valid_biomarker_columns) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    df['biomarkers'] = 0\n",
    "    print(\"No valid biomarker columns found in the dataset. Setting 'biomarkers' to 0.\")\n",
    "\n",
    "\n",
    "# Define comorbidity columns\n",
    "comorbidity_columns = [\n",
    "    'alcohol_elixhauser_tm1', 'anemia_elixhauser_tm1', 'arrhythmia_elixhauser_tm1',\n",
    "    'arthritis_elixhauser_tm1', 'bloodlossanemia_elixhauser_tm1', 'coagulopathy_elixhauser_tm1',\n",
    "    'compdiabetes_elixhauser_tm1', 'depression_elixhauser_tm1', 'drugabuse_elixhauser_tm1',\n",
    "    'electrolytes_elixhauser_tm1', 'hypertension_elixhauser_tm1', 'hypothyroid_elixhauser_tm1',\n",
    "    'liver_elixhauser_tm1', 'neurodegen_elixhauser_tm1', 'obesity_elixhauser_tm1',\n",
    "    'paralysis_elixhauser_tm1', 'psychosis_elixhauser_tm1', 'pulmcirc_elixhauser_tm1',\n",
    "    'pvd_elixhauser_tm1', 'renal_elixhauser_tm1', 'uncompdiabetes_elixhauser_tm1',\n",
    "    'valvulardz_elixhauser_tm1', 'wtloss_elixhauser_tm1', 'cerebrovasculardz_romano_tm1',\n",
    "    'chf_romano_tm1', 'dementia_romano_tm1', 'hemiplegia_romano_tm1', 'hivaids_romano_tm1',\n",
    "    'metastatic_romano_tm1', 'myocardialinfarct_romano_tm1', 'pulmonarydz_romano_tm1',\n",
    "    'tumor_romano_tm1', 'ulcer_romano_tm1'\n",
    "]\n",
    "\n",
    "# Select columns where data is present (non-null values)\n",
    "non_empty_comorbidity_columns = df[comorbidity_columns].dropna(axis=1, how='all')\n",
    "\n",
    "# Randomly select 10 columns with data present\n",
    "if len(non_empty_comorbidity_columns.columns) < 10:\n",
    "    random_10_comorbidity_columns = non_empty_comorbidity_columns.columns.tolist()\n",
    "    print(f\"Only {len(random_10_comorbidity_columns)} comorbidity columns available. Using all.\")\n",
    "else:\n",
    "    random_10_comorbidity_columns = non_empty_comorbidity_columns.sample(n=10, axis=1, random_state=42).columns.tolist()\n",
    "\n",
    "# Ensure selected comorbidity columns exist in the DataFrame\n",
    "valid_comorbidity_columns = [col for col in random_10_comorbidity_columns if col in df.columns]\n",
    "\n",
    "# Create the 'comorbidity' column\n",
    "if valid_comorbidity_columns:\n",
    "    # Example consolidation: set to 1 if any comorbidity is present\n",
    "    df['comorbidity'] = df[valid_comorbidity_columns].apply(\n",
    "        lambda row: 1 if any(row[col] > 0 for col in valid_comorbidity_columns) else 0,\n",
    "        axis=1\n",
    "    )\n",
    "else:\n",
    "    df['comorbidity'] = 0\n",
    "    print(\"No valid comorbidity columns found in the dataset. Setting 'comorbidity' to 0.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature and Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to be used for prediction\n",
    "features = [\n",
    "    'age', 'dem_female', 'race', 'biomarkers', 'comorbidity',\n",
    "    'lasix_dose_count_tm1', 'cre_tests_tm1', 'crp_tests_tm1', 'esr_tests_tm1',\n",
    "    'ghba1c_tests_tm1', 'hct_tests_tm1', 'ldl_tests_tm1', 'nt_bnp_tests_tm1',\n",
    "    'sodium_tests_tm1', 'trig_tests_tm1'\n",
    "]\n",
    "\n",
    "# Check that all features are available in the dataset\n",
    "missing_features = [feature for feature in features if feature not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Missing features from the dataset: {missing_features}\")\n",
    "\n",
    "\n",
    "# Define the target variables\n",
    "target_columns = ['risk_score_t', 'cost_t', 'bps_mean_t', 'gagne_sum_t', 'ldl_mean_t']\n",
    "\n",
    "# Check that all target columns exist\n",
    "missing_targets = [target for target in target_columns if target not in df.columns]\n",
    "if missing_targets:\n",
    "    print(f\"Missing target columns from the dataset: {missing_targets}\")\n",
    "    raise KeyError(f\"The following target columns are missing: {missing_targets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- risk_score_t ---\n",
      "Mean Squared Error: 18.927368668148475\n",
      "R-squared: 0.3315504066485432\n",
      "\n",
      "Feature Importance for 'risk_score_t':\n",
      "                 Feature  Importance\n",
      "6          cre_tests_tm1    0.331480\n",
      "0                    age    0.116470\n",
      "10         hct_tests_tm1    0.110542\n",
      "4            comorbidity    0.088552\n",
      "13      sodium_tests_tm1    0.082573\n",
      "9       ghba1c_tests_tm1    0.053679\n",
      "8          esr_tests_tm1    0.041045\n",
      "14        trig_tests_tm1    0.037929\n",
      "12      nt_bnp_tests_tm1    0.031657\n",
      "11         ldl_tests_tm1    0.030893\n",
      "1             dem_female    0.029604\n",
      "2                   race    0.023662\n",
      "5   lasix_dose_count_tm1    0.021110\n",
      "7          crp_tests_tm1    0.000805\n",
      "3             biomarkers    0.000000\n",
      "\n",
      "\n",
      "Model training and evaluation completed successfully.\n",
      "--- cost_t ---\n",
      "Mean Squared Error: 318659790.500639\n",
      "R-squared: 0.014538118619836782\n",
      "\n",
      "Feature Importance for 'cost_t':\n",
      "                 Feature  Importance\n",
      "6          cre_tests_tm1    0.174385\n",
      "0                    age    0.153828\n",
      "10         hct_tests_tm1    0.129224\n",
      "9       ghba1c_tests_tm1    0.097446\n",
      "13      sodium_tests_tm1    0.091458\n",
      "11         ldl_tests_tm1    0.057799\n",
      "14        trig_tests_tm1    0.057166\n",
      "8          esr_tests_tm1    0.055875\n",
      "5   lasix_dose_count_tm1    0.046513\n",
      "1             dem_female    0.043361\n",
      "2                   race    0.037774\n",
      "12      nt_bnp_tests_tm1    0.028622\n",
      "4            comorbidity    0.025631\n",
      "7          crp_tests_tm1    0.000919\n",
      "3             biomarkers    0.000000\n",
      "\n",
      "\n",
      "Model training and evaluation completed successfully.\n",
      "--- bps_mean_t ---\n",
      "Mean Squared Error: 334.54491009714326\n",
      "R-squared: -0.003975577857832091\n",
      "\n",
      "Feature Importance for 'bps_mean_t':\n",
      "                 Feature  Importance\n",
      "0                    age    0.200674\n",
      "10         hct_tests_tm1    0.139932\n",
      "6          cre_tests_tm1    0.099523\n",
      "13      sodium_tests_tm1    0.096317\n",
      "9       ghba1c_tests_tm1    0.085675\n",
      "1             dem_female    0.072795\n",
      "8          esr_tests_tm1    0.064653\n",
      "14        trig_tests_tm1    0.063822\n",
      "11         ldl_tests_tm1    0.063506\n",
      "4            comorbidity    0.045139\n",
      "2                   race    0.028183\n",
      "12      nt_bnp_tests_tm1    0.022838\n",
      "5   lasix_dose_count_tm1    0.015517\n",
      "7          crp_tests_tm1    0.001427\n",
      "3             biomarkers    0.000000\n",
      "\n",
      "\n",
      "Model training and evaluation completed successfully.\n",
      "--- gagne_sum_t ---\n",
      "Mean Squared Error: 2.19527215519945\n",
      "R-squared: 0.4093084254560123\n",
      "\n",
      "Feature Importance for 'gagne_sum_t':\n",
      "                 Feature  Importance\n",
      "4            comorbidity    0.348375\n",
      "6          cre_tests_tm1    0.148429\n",
      "0                    age    0.098254\n",
      "13      sodium_tests_tm1    0.088527\n",
      "10         hct_tests_tm1    0.071717\n",
      "9       ghba1c_tests_tm1    0.068573\n",
      "8          esr_tests_tm1    0.030941\n",
      "11         ldl_tests_tm1    0.030435\n",
      "14        trig_tests_tm1    0.030433\n",
      "1             dem_female    0.027625\n",
      "12      nt_bnp_tests_tm1    0.022836\n",
      "2                   race    0.018367\n",
      "5   lasix_dose_count_tm1    0.014738\n",
      "7          crp_tests_tm1    0.000751\n",
      "3             biomarkers    0.000000\n",
      "\n",
      "\n",
      "Model training and evaluation completed successfully.\n",
      "--- ldl_mean_t ---\n",
      "Mean Squared Error: 459.07297716530104\n",
      "R-squared: -0.06138382858397362\n",
      "\n",
      "Feature Importance for 'ldl_mean_t':\n",
      "                 Feature  Importance\n",
      "10         hct_tests_tm1    0.157374\n",
      "0                    age    0.130014\n",
      "6          cre_tests_tm1    0.117300\n",
      "13      sodium_tests_tm1    0.115766\n",
      "9       ghba1c_tests_tm1    0.100805\n",
      "8          esr_tests_tm1    0.072078\n",
      "11         ldl_tests_tm1    0.056939\n",
      "4            comorbidity    0.056806\n",
      "14        trig_tests_tm1    0.055816\n",
      "2                   race    0.045980\n",
      "1             dem_female    0.039691\n",
      "12      nt_bnp_tests_tm1    0.029840\n",
      "5   lasix_dose_count_tm1    0.018002\n",
      "7          crp_tests_tm1    0.003589\n",
      "3             biomarkers    0.000000\n",
      "\n",
      "\n",
      "Model training and evaluation completed successfully.\n",
      "Cannot proceed with model training due to missing features or target columns.\n"
     ]
    }
   ],
   "source": [
    "# Proceed only if no features or targets are missing\n",
    "if not missing_features and not missing_targets:\n",
    "    # Prepare the features (X) and targets (y)\n",
    "    X = df[features]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Save the scaler\n",
    "    with open('standard_scaler.pkl', 'wb') as scaler_file_out:\n",
    "        pickle.dump(scaler, scaler_file_out)\n",
    "\n",
    "    # Compress the scaler\n",
    "    with open('standard_scaler.pkl', 'rb') as scaler_file_in:\n",
    "        with gzip.open('standard_scaler.pkl.gz', 'wb') as scaler_file_out_compressed:\n",
    "            scaler_file_out_compressed.write(scaler_file_in.read())\n",
    "\n",
    "    # Initialize and train the MultiOutput Random Forest model\n",
    "    model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Evaluate the model for each target and get feature importance\n",
    "for idx, target in enumerate(target_columns):\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test.iloc[:, idx], y_pred[:, idx])\n",
    "    r2 = r2_score(y_test.iloc[:, idx], y_pred[:, idx])\n",
    "    print(f\"--- {target} ---\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R-squared: {r2}\\n\")\n",
    "    \n",
    "    # Get feature importance for the current target\n",
    "    feature_importance = model.estimators_[idx].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    print(f\"Feature Importance for '{target}':\")\n",
    "    print(feature_importance_df)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    # Save the multi-output random forest model\n",
    "    with open('multi_output_random_forest_model.pkl', 'wb') as model_file_out:\n",
    "        pickle.dump(model, model_file_out)\n",
    "\n",
    "    # Compress the model\n",
    "    with open('multi_output_random_forest_model.pkl', 'rb') as model_file_in:\n",
    "        with gzip.open('multi_output_random_forest_model_compressed.pkl.gz', 'wb') as model_file_out_compressed:\n",
    "            model_file_out_compressed.write(model_file_in.read())\n",
    "\n",
    "    print(\"Model training and evaluation completed successfully.\")\n",
    "else:\n",
    "    print(\"Cannot proceed with model training due to missing features or target columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  dem_female  race  biomarkers  comorbidity  lasix_dose_count_tm1  \\\n",
      "0   57           0     1           1            1                     0   \n",
      "\n",
      "   cre_tests_tm1  crp_tests_tm1  esr_tests_tm1  ghba1c_tests_tm1  \\\n",
      "0            100              2              1                 3   \n",
      "\n",
      "   hct_tests_tm1  ldl_tests_tm1  nt_bnp_tests_tm1  sodium_tests_tm1  \\\n",
      "0              1              2                 0                 1   \n",
      "\n",
      "   trig_tests_tm1  \n",
      "0               2  \n",
      "Selected target(s) for prediction: ['bps_mean_t']\n"
     ]
    }
   ],
   "source": [
    "# Load the scaler\n",
    "with open('standard_scaler.pkl', 'rb') as scaler_file_in:\n",
    "    scaler = pickle.load(scaler_file_in)\n",
    "\n",
    "# Load the model\n",
    "with open('multi_output_random_forest_model.pkl', 'rb') as model_file_in:\n",
    "    model = pickle.load(model_file_in)\n",
    "\n",
    "\n",
    "# Define a sample input DataFrame for prediction\n",
    "sample_input = pd.DataFrame({\n",
    "    'age': [57],               # Example age\n",
    "    'dem_female': [0],         # Male\n",
    "    'race': [1],               # Encoded race\n",
    "    'biomarkers': [1],         # Biomarker status\n",
    "    'comorbidity': [1],        # Comorbidity status\n",
    "    'lasix_dose_count_tm1': [0],   # Example value\n",
    "    'cre_tests_tm1': [100],         # Example value\n",
    "    'crp_tests_tm1': [2],           # Example value\n",
    "    'esr_tests_tm1': [1],           # Example value\n",
    "    'ghba1c_tests_tm1': [3],        # Example value\n",
    "    'hct_tests_tm1': [1],           # Example value\n",
    "    'ldl_tests_tm1': [2],           # Example value\n",
    "    'nt_bnp_tests_tm1': [0],        # Example value\n",
    "    'sodium_tests_tm1': [1],        # Example value\n",
    "    'trig_tests_tm1': [2]           # Example value\n",
    "})\n",
    "\n",
    "print(sample_input)\n",
    "\n",
    "# Specify desired targets for prediction\n",
    "# Choices are: 'risk_score_t', 'cost_t','bps_mean_t', 'gagne_sum_t'\n",
    "\n",
    "desired_targets = ['bps_mean_t']  # Example: Predict only 'risk_score_t'\n",
    "\n",
    "# Validate desired targets\n",
    "valid_targets = [target for target in desired_targets if target in target_columns]\n",
    "invalid_targets = [target for target in desired_targets if target not in target_columns]\n",
    "\n",
    "if invalid_targets:\n",
    "    print(f\"Invalid target(s) specified: {invalid_targets}\")\n",
    "    print(f\"Valid targets are: {target_columns}\")\n",
    "else:\n",
    "    print(f\"Selected target(s) for prediction: {valid_targets}\")\n",
    "\n",
    "# Define feature importances for each target (this should be based on your actual model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions Based on Selected Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bps_mean_t    132.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if not invalid_targets:\n",
    "    # Ensure the sample input has all required features\n",
    "    if all(feature in sample_input.columns for feature in features):\n",
    "        # Scale the sample input\n",
    "        X_sample_scaled = scaler.transform(sample_input[features])\n",
    "\n",
    "        # Make predictions\n",
    "        predicted_values = model.predict(X_sample_scaled)[0]\n",
    "        prediction_series = pd.Series(predicted_values, index=target_columns)\n",
    "\n",
    "        # Select only the desired targets\n",
    "        selected_predictions = prediction_series[valid_targets]\n",
    "\n",
    "        print(selected_predictions)\n",
    "\n",
    "    else:\n",
    "        missing_in_input = [feature for feature in features if feature not in sample_input.columns]\n",
    "        print(f\"Sample input is missing required features: {missing_in_input}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
